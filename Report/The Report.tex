\documentclass[a4paper, 12pt]{article}
\usepackage[pdftex]{graphicx}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage[all]{hypcap}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumitem}
\title{CS201 - Data Structures \\ Huffman Coding}
\author{}

\newcommand{\mat}[1]{\boldsymbol { \mathsf{#1}} }

\begin{document}
\setlength{\parskip}{10pt}
\setlength{\parindent}{0pt}
\DeclareGraphicsExtensions{.pdf,.png,.gif,.jpg}
\maketitle

\newpage

\section{Members}
\begin{itemize}
	\item Fatima Nadeem
	\item Kabir Kumar
	\item Muhammad Shahrom Ali
\end{itemize}


\section{Introduction}
	Most algorithms we see are focused on reducing time taken by a computational task to as little as possible and looking at space usage as a secondary problem. 
	File compression is where we take a seperate path; we try to reduce the space consumed and in as little time as possible. 
	This project emphasizes on one particular type of file compression technique known as \textit{Huffman Coding} that uses variable-length encoding of characters to reduce the number of bits (and consequentially, storage) occupied by our data as a total. 

\section{Variable-Length Encoding}

\section{Implementations}

\begin{enumerate}
	\item Binary Tree Based (Heap) Implementation 
	
	\item Sorted-List Based Implementation 
\end{enumerate}


\section{Data Set}
	For the data set, we took portions from text books and novels. Starting from 500 characters, then 1000 characters, 10,000 characters, 100,000 characters. To further smooth our analysis we can add data sets of sizes 20,000, 50,000, and 200,000 characters. 
	For each of these sets, we will time the encoding operation and see how much space has been reduced. 

\section{Analysis}

%insert table of Values here 

\section{Conclusion}

\section{References}
	\begin{enumerate}
		\item https://www.techiedelight.com/huffman-coding/
		\item https://en.wikipedia.org/wiki/Huffman\_coding
		\item \textit{Algorithms in C++} -  Sedgewick, Robert
	\end{enumerate}
\end{document}